import datetime
import os
import pickle
import sys
import time
import warnings
from itertools import tee
from logging import getLogger

import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix, load_npz, save_npz, vstack
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import (GroupKFold, GroupShuffleSplit,
                                     StratifiedKFold)
from sklearn.preprocessing import OneHotEncoder
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import DataLoader,TensorDataset
from torch.nn.functional import binary_cross_entropy
from torch.optim import Adam

from tools.utils.encoding import fill_unseens, label_encoding
# import tools.models.my_lightgbm as mlgb
from tools.utils.feature_tools import get_all_features, load_features
from tools.utils.foldings import TimeSplitSingleFold
from tools.utils.general_utils import (dec_timer, get_locs, load_configs,
                                       log_evaluation, logInit, parse_args,
                                       sel_log, send_line_notification,
                                       test_commit)
from tools.utils.metrics import eval_auc
from tools.utils.visualizations import save_importance

from tools.models.embedding_nn import seed_everything, embed_features_split, SimpleEmbeddingNet
from tools.utils.scalers import MKRankGaussScalar

warnings.simplefilter(action='ignore', category=FutureWarning)


@dec_timer
def train(args, logger):
    '''
    policy
    ------------
    * use original functions only if there's no pre-coded functions
        in useful libraries such as sklearn.

    todos
    ------------
    * load features
    * train the model
    * save the followings
        * logs
        * oofs
        * importances
        * trained models
        * submissions (if test mode)

    '''
    # -- Prepare for training
    exp_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
    nes_dir = './inputs/nes_info/'
    feature_dir = './inputs/features/'
    configs = load_configs('./config.yml', logger)

    # -- Load train data
    sel_log('loading training data ...', None)
    trn_ids = pd.read_pickle(
        nes_dir + 'trn_MachineIdentifier.pkl.gz', compression='gzip')
    tst_ids = pd.read_pickle(
        nes_dir + 'tst_MachineIdentifier.pkl.gz', compression='gzip')
    target = pd.read_pickle(
        nes_dir + 'HasDetections.pkl.gz', compression='gzip')
    val_flgs = pd.read_pickle(nes_dir + 'val_flg.pkl')
    if configs['train']['debug']:
        sample_idxes = trn_ids.reset_index(
            drop=True).sample(
            random_state=71,
            frac=0.05).index
        target = target.iloc[sample_idxes].reset_index(drop=True)
        trn_ids = trn_ids.iloc[sample_idxes].reset_index(drop=True)
        val_flgs = val_flgs.iloc[sample_idxes].reset_index(drop=True)

    if configs['train']['all_features']:
        _features = get_all_features('./inputs/features/')
    else:
        _features = configs['nn_features']
    trn_tst_df = load_features(_features, feature_dir, args.nthread, logger)\
        .set_index('MachineIdentifier')

    # feature selection if needed
#    if configs['train']['feature_selection']:
#        features_df = select_features(features_df,
#                                      configs['train']['feature_select_path'],
#                                      configs['train']['metric'],
#                                      configs['train']['feature_topk'])
#        test_features_df = select_features(test_features_df,
#                                           configs['train']['feature_select_path'],
#                                           configs['train']['metric'],
#                                           configs['train']['feature_topk'])

    features = trn_tst_df.columns
    # clarify the used categorical features
    # also encoding categorical features
    if configs['categorical_features']:
        categorical_features = sorted(
            list(set(configs['nn_categorical_features']) & set(features)))
    else:
        categorical_features = []

    sel_log('now scaling and encoding ...', logger)
    seed_everything()
    embed_cols, len_embed_cols, other_cols = embed_features_split(trn_tst_df, categorical_features)
#    ec = []
#    lec = []
#    for col, col_len in zip(embed_cols, len_embed_cols):
#        if col_len > 2:
#            ec.append(col)
#            lec.append(col_len)
#        elif col_len == 2:
#            print(f'WARNING: binary col, {col} !!!!!!!!!!!!!!!!!!!!!!!!!!')
#            other_cols.append(col)
#    embed_cols = ec
#    len_embed_cols = lec
    # sort_cols
    trn_tst_df = trn_tst_df.loc[:, embed_cols + other_cols]
    # scale numericals
#    trn_tst_df_numerical = trn_tst_df.loc[:, other_cols]
#    scaler = MKRankGaussScalar()
#    scaler.fit(trn_tst_df_numerical)
#    trn_tst_df.loc[:, other_cols] = scaler.transform(trn_tst_df_numerical)
#    for col in trn_tst_df_numerical.columns:
#        temp = trn_tst_df[col].replace(np.inf, np.nan)
#        temp = temp.fillna(float(temp.mean()))
#        temp = temp.astype('float32')
#        trn_tst_df.loc[:, col] = temp
#    for col in embed_cols:
#        temp = trn_tst_df[col].replace(np.inf, np.nan)
#        temp = temp.fillna(int(temp.mode()))
#        temp = temp.astype('int32')
#        trn_tst_df.loc[:, col] = temp
#    # encode cat cols
#    cat_df, _ = label_encoding(trn_tst_df.loc[:, embed_cols], fit_columns=embed_cols)
#    trn_tst_df.loc[:, embed_cols] = cat_df

    # split train and test
    sel_log(f'now splitting the df to train and test ones ...', None)
    features_df = trn_tst_df.loc[trn_ids].reset_index(drop=True)
    test_features_df = trn_tst_df.loc[tst_ids].reset_index(drop=True)

    # remove invalid features
###    features_df.drop(configs['invalid_features'], axis=1, inplace=True)

    # categorical_features = get_locs(
    #     features_df, configs['categorical_features'])

    # -- Split using group k-fold w/ shuffling
    # NOTE: this is not stratified, I wanna implement it in the future
    if configs['train']['fold_type'] == 'skf':
        skf = StratifiedKFold(configs['train']['fold_num'], random_state=71)
        folds = skf.split(features_df, target)
        configs['train']['single_model'] = False
        if 'e005_avsig_ts_diff' in features_df.columns:
            features_df['e005_avsig_ts_diff'] = pd.read_pickle(
                'inputs/nes_info/e005_avsig_ts_diff_full_trn.pkl.gz', compression='gzip')
        if 'e005_osver_ts_diff' in features_df.columns:
            features_df['e005_osver_ts_diff'] = pd.read_pickle(
                'inputs/nes_info/e005_osver_ts_diff_full_trn.pkl.gz', compression='gzip')
        if 'e008_datebl_ts_diff' in features_df.columns:
            features_df['e008_datebl_ts_diff'] = pd.read_pickle(
                'inputs/nes_info/e008_datebl_ts_diff_full_trn.pkl.gz', compression='gzip')
    elif configs['train']['fold_type'] == 'tssf':
        tssf = TimeSplitSingleFold()
        folds = tssf.split(val_flgs)
        configs['train']['single_model'] = True
    else:
        print(f"ERROR: wrong fold_type, {configs['train']['fold_type']}")
    folds, pred_folds = tee(folds)

    # ohe
    if configs['train']['ohe']:
        sel_log('one hot encoding!', None)
        # drop categories
        # for col in categorical_features:
        for col in features_df.columns:
            features_df.loc[features_df[col].isnull().values
                            | (features_df[col] == np.inf).values,
                            col] = features_df[col].mode().values
            features_df[col] = features_df[col].astype(int)
        cat_place = [
            i for i in range(features_df.shape[1])
            if features_df.columns[i] in categorical_features]
        # fit one hot encoder
        ohe = OneHotEncoder(
            categorical_features=cat_place,
            sparse=False,
            dtype='uint8')
        ohe.fit(features_df)
        features_df = pd.DataFrame(ohe.transform(features_df))
        # only used for test
        if args.test:
            sel_log('test one hot encoding!', None)
            for col in test_features_df.columns:
                test_features_df.loc[test_features_df[col].isnull().values
                                     | (test_features_df[col] == np.inf).values,
                                     col] = test_features_df[col].mode().values
                test_features_df[col] = test_features_df[col].astype(int)
            test_features_df = pd.DataFrame(ohe.transform(test_features_df))
        categorical_features = 'auto'

    # print shape
    sel_log(f'the shape features_df is {features_df.shape}', logger)

    # -- CV

    sel_log('start training ...', None)
    cv_model = []
    for i, idxes in tqdm(list(enumerate(folds))):
        trn_idx, val_idx = idxes
        # -- Data resampling
        # Stock original data for validation
        X_train, X_val = features_df.iloc[trn_idx], features_df.iloc[val_idx]
        y_train, y_val = target[trn_idx], target[val_idx]
        torch_X_train = torch.FloatTensor(X_train.values)
        torch_X_val = torch.FloatTensor(X_val.values)
        torch_y_train = torch.FloatTensor(y_train.values.astype(np.int32))
        torch_y_val = torch.FloatTensor(y_val.values.astype(np.int32))

        batch_size = 512
        n_epochs = 10
        model = SimpleEmbeddingNet(embed_cols, len_embed_cols, other_cols)
        loss_fn = torch.nn.BCELoss(reduction='mean')
        optimizer = Adam(model.parameters())
        torch_train = torch.utils.data.TensorDataset(torch_X_train, torch_y_train)
        train_loader = torch.utils.data.DataLoader(torch_train, batch_size=batch_size, shuffle=True)
        torch_val = torch.utils.data.TensorDataset(torch_X_val, torch_y_val)
        valid_loader = torch.utils.data.DataLoader(torch_val, batch_size=batch_size, shuffle=False)

        valid_preds = np.zeros((torch_X_val.size(0)))

        model.cuda()
#        model = torch.nn.DataParallel(model) # make parallel
        for epoch in range(n_epochs): 
            print(f'------ epoch {epoch} ------')
            start_time = time.time()
            avg_loss = 0.
            model.train()
            for x_batch, y_batch in tqdm(train_loader, disable=False):
                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()
                # Forward pass: compute predicted y by passing x to the model.
                y_pred = model(x_batch)
                # Compute and print loss.
                loss = loss_fn(y_pred, y_batch)
                # Before the backward pass, use the optimizer object to zero all of the
                # gradients for the Tensors it will update (which are the learnable weights
                # of the model)
                optimizer.zero_grad()
                # Backward pass: compute gradient of the loss with respect to model parameters
                loss.backward()
                # Calling the step function on an Optimizer makes an update to its parameters
                optimizer.step()
                avg_loss += loss.item() / len(train_loader)
        
            # set evaluation mode of the model. This disabled operations which are only applied during training like dropout
            model.eval()
    
            avg_val_loss = 0.
            for i, (x_batch, y_batch) in enumerate(valid_loader):
                # detach returns a new Tensor, detached from the current graph whose result will never require gradient
                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()
                # Forward pass: compute predicted y by passing x to the model.
                # y_val_pred = model(x_batch.cuda()).detach()
                y_val_pred = model(x_batch).detach()
                avg_val_loss += loss_fn(y_val_pred, y_batch).item() / len(valid_loader)
                valid_preds[i * batch_size:(i+1) * batch_size] = y_val_pred.cpu().numpy()[:, 0]
            elapsed_time = time.time() - start_time 
            logger.info('\nEpoch {}/{} \t loss={:.4f} \t val_loss={:.4f} \t time={:.2f}s'.format(
                epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))
            auc_mean = roc_auc_score(torch_y_val.cpu(),valid_preds).round(4)
            logger.info('AUC_VAL{} '.format(auc_mean))
    model.cpu()

    # -- Post processings
    filename_base = f'{args.exp_ids[0]}_{exp_time}_{auc_mean:.4}'

    # --- Make submission file
    if args.test:
        if configs['train']['single_model']:
            # reload features of timediff from the last day of each dataset
            if 'e005_avsig_ts_diff' in features_df.columns:
                features_df['e005_avsig_ts_diff'] = pd.read_pickle(
                    'inputs/nes_info/e005_avsig_ts_diff_full_trn.pkl.gz', compression='gzip')
            if 'e005_osver_ts_diff' in features_df.columns:
                features_df['e005_osver_ts_diff'] = pd.read_pickle(
                    'inputs/nes_info/e005_osver_ts_diff_full_trn.pkl.gz', compression='gzip')
            if 'e008_datebl_ts_diff' in features_df.columns:
                features_df['e008_datebl_ts_diff'] = pd.read_pickle(
                    'inputs/nes_info/e008_datebl_ts_diff_full_trn.pkl.gz', compression='gzip')
            # train single model
            n_epochs = int(n_epochs * 1.25)
            X_train = features_df
            y_train = target
            torch_X_train = torch.FloatTensor(X_train.values)
            torch_y_train = torch.FloatTensor(y_train.values.astype(np.int32))
    
            batch_size = 512
            model = SimpleEmbeddingNet(embed_cols, len_embed_cols, other_cols)
            loss_fn = torch.nn.BCELoss(reduction='mean')
            optimizer = Adam(model.parameters())
            torch_train = torch.utils.data.TensorDataset(torch_X_train, torch_y_train)
            train_loader = torch.utils.data.DataLoader(torch_train, batch_size=batch_size, shuffle=True)
    
            model.train()
            model.cuda()
#            model = torch.nn.DataParallel(model) # make parallel
            for epoch in range(n_epochs): 
                print(f'------ epoch {epoch} ------')
                for x_batch, y_batch in tqdm(train_loader, disable=False):
                    x_batch, y_batch = x_batch.cuda(), y_batch.cuda()
                    # Forward pass: compute predicted y by passing x to the model.
                    y_pred = model(x_batch)
                    # Compute and print loss.
                    loss = loss_fn(y_pred, y_batch)
                    # Before the backward pass, use the optimizer object to zero all of the
                    # gradients for the Tensors it will update (which are the learnable weights
                    # of the model)
                    optimizer.zero_grad()
                    # Backward pass: compute gradient of the loss with respect to model parameters
                    loss.backward()
                    # Calling the step function on an Optimizer makes an update to its parameters
                    optimizer.step()
                    avg_loss += loss.item() / len(train_loader)
        
            # set evaluation mode of the model. This disabled operations which are only applied during training like dropout
            model.eval()

            # re-save model for prediction
            cv_model = [model]

        #        # -- Prepare for test
        #        test_base_dir = './inputs/test/'
        #
        #        sel_log('loading test data ...', None)
        #        test_features_df = load_features(
        #            features, test_base_dir, logger)
        #        # label encoding
        #        sel_log('encoding categorical features ...', None)
        #        test_features_df = fill_unseens(features_df, test_features_df,
        #                                        configs['categorical_features'],
        #                                        args.nthread)
        #        test_features_df, le_dict = label_encoding(test_features_df, le_dict)

        # -- Prediction
        sel_log('predicting for test ...', None)
        preds = []
        # for booster in tqdm(cv_model.boosters):
        torch_test  = torch.FloatTensor(test_features_df.values)
        torch_test = torch.utils.data.TensorDataset(torch_test)
        test_loader = torch.utils.data.DataLoader(torch_test, batch_size=batch_size, shuffle=False)
        test_preds = np.zeros((len(torch_test)))

        for model in tqdm(cv_model):
            for i, (x_batch,) in tqdm(enumerate(test_loader)):
                # y_pred = model(x_batch).detach()
                y_pred = model(x_batch.cuda()).detach()
                test_preds[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()[:, 0]
            pred = pd.Series(test_preds)
            preds.append(pred.rank() / pred.shape)

        if len(cv_model) > 1:
            target_values = np.mean(preds, axis=0)
        else:
            target_values = preds[0]

        # -- Make submission file
        sel_log(f'loading sample submission file ...', None)
        sub_df = pd.read_csv(
            './inputs/origin/sample_submission.csv.zip',
            compression='zip')
        sub_df.HasDetections = target_values

        # print stats
        submission_filename = f'./submissions/{filename_base}_sub.csv.gz'
        sel_log(f'saving submission file to {submission_filename}', logger)
        sub_df.to_csv(submission_filename, compression='gzip', index=False)
        if args.submit:
            os.system(
                f'kaggle competitions submit microsoft-malware-prediction -f {submission_filename} -m "{args.message}"')


if __name__ == '__main__':
    t0 = time.time()
    logger = getLogger(__name__)
    logger = logInit(logger, './logs/', 'lgb_train.log')
    args = parse_args(logger)

    logger.info('')
    logger.info('')
    logger.info(
        f'============ EXP {args.exp_ids[0]}, START TRAINING =============')
    train(args, logger)
    test_commit(args, './logs/test_commit.log')
    prec_time = time.time() - t0
    send_line_notification(
        f'Finished: {" ".join(sys.argv)} in {prec_time:.1f} s !')
