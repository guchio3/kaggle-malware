import gc
import time
import os
import random
import time
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import auc, roc_auc_score, roc_curve
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import MinMaxScaler
from torch.nn.functional import binary_cross_entropy
from torch.optim import Adam
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

pd.set_option('display.max_columns', 500)
warnings.filterwarnings('ignore')


def seed_everything(seed=1234):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True


def embed_features_split(df, cat_cols):
    embed_cols = []
    len_embed_cols = []
    other_cols = []
    for col in df.columns:
        if col in cat_cols:
            embed_cols.append(col)
            len_embed_cols.append(df[col].nunique())
        else:
            other_cols.append(col)
    return embed_cols, len_embed_cols, other_cols


class SimpleEmbeddingNet(nn.Module):
    def __init__(self, embed_cols, len_embed_cols, other_cols):
        super().__init__()
        self.emb_layers = nn.ModuleList()
        self.dropout = nn.Dropout(.25)
        self.num_categorical = len(len_embed_cols)
        self.num_numeric = len(other_cols)
        
        for embed_col, len_embed_col in zip(embed_cols, len_embed_cols):
            self.emb_layers.append(nn.Embedding(len_embed_col, len_embed_col // 2))

        ff_inp_dim = sum(e.embedding_dim for e in self.emb_layers) + self.num_numeric
        self.ff = nn.Sequential(
            nn.Linear(ff_inp_dim, 128*2),
            nn.ELU(),
            # nn.ReLU(),
            nn.Dropout(p=.25),
            nn.Linear(128*2, 64*2),
            nn.ELU(),
            # nn.ReLU(),
            nn.Dropout(p=.25),
            nn.Linear(64*2, 32*2),
            nn.ELU(),
            # nn.ReLU(),
            nn.Dropout(p=.25),
            nn.Linear(32*2, 1),
            nn.Sigmoid()
        )

    def forward(self, x_batch):
        emb_indices = x_batch[:, :self.num_categorical].long()
        emb_outs = []
        for i, emb_layer in enumerate(self.emb_layers):
            emb_out = emb_layer(emb_indices[:, i])
            emb_out = self.dropout(emb_out)
#            print(f'e: {i}', flush=True)
            emb_outs.append(emb_out)
        
        embs = torch.cat(emb_outs, dim=1)

        x_numerical = x_batch[:, self.num_categorical:]
        embs_num = torch.cat([embs, x_numerical], dim=1)
        out = self.ff(embs_num)
        return out


if __name__ == '__main__':
    sen = SimpleEmbeddingNet([], [], [])
